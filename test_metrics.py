#!/usr/bin/env python3
"""
Test script to force model training and generate performance metrics
"""

import sys
import os
from app import WebSecurityMonitor
import logging

# Logging ayarÄ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_model_training():
    """Model eÄŸitimini zorla tetikle ve performans metriklerini oluÅŸtur"""
    
    # Monitor oluÅŸtur
    monitor = WebSecurityMonitor("access.log")
    
    # model_trained'i False yap
    monitor.model_trained = False
    
    # Log dosyasÄ±nÄ±n baÅŸÄ±ndan okumaya zorla
    monitor.last_read_position = 0
    
    logger.info("Test: Model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
    
    # Tehdit tespitini Ã§alÄ±ÅŸtÄ±r (bu model eÄŸitimini tetikleyecek)
    threats = monitor.detect_threats()
    
    logger.info(f"Test: {len(threats)} tehdit tespit edildi")
    
    # Models klasÃ¶rÃ¼nÃ¼ kontrol et
    if os.path.exists("models/performance_metrics.json"):
        logger.info("âœ… Performans metrikleri dosyasÄ± oluÅŸturuldu!")
        
        # Dosya iÃ§eriÄŸini gÃ¶ster
        with open("models/performance_metrics.json", 'r', encoding='utf-8') as f:
            content = f.read()
            print("\nğŸ¯ Performans Metrikleri:")
            print("=" * 50)
            print(content)
            print("=" * 50)
    else:
        logger.warning("âŒ Performans metrikleri dosyasÄ± oluÅŸturulamadÄ±")
    
    # Model dosyalarÄ±nÄ± kontrol et
    model_files = ["lstm_model.h5", "rf_model.pkl", "scaler.pkl", "features.json"]
    for model_file in model_files:
        path = f"models/{model_file}"
        if os.path.exists(path):
            logger.info(f"âœ… {model_file} oluÅŸturuldu")
        else:
            logger.warning(f"âŒ {model_file} oluÅŸturulamadÄ±")

if __name__ == "__main__":
    test_model_training() 